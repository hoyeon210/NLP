{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "845254cf-a600-43bf-9d02-01a18f72f4b8",
   "metadata": {},
   "source": [
    "### word2vec을 이용한 모델\n",
    "- word2vec은 단어로 표현된 리스트를 입력값으로 넣어야 함\n",
    "- 전처리된 텍스트를 불러온 후 각 단어들의 리스트로 나누어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ab4712-2090-417a-b0bc-e54e074adeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9072953e-68a4-429f-b54c-dc53251b8557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stuff going moment mj started listening music ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>classic war worlds timothy hines entertaining ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>film starts manager nicholas bell giving welco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>must assumed praised film greatest filmed oper...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>superbly trashy wondrously unpretentious explo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  stuff going moment mj started listening music ...          1\n",
       "1  classic war worlds timothy hines entertaining ...          1\n",
       "2  film starts manager nicholas bell giving welco...          0\n",
       "3  must assumed praised film greatest filmed oper...          0\n",
       "4  superbly trashy wondrously unpretentious explo...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_IN_PATH = './data_in/'\n",
    "DATA_OUT_PATH = './data_out/'\n",
    "TRAIN_CLEAN_DATA = 'train_clean.npy'\n",
    "\n",
    "train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2996460a-3a91-4593-a760-2509d53d0b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = list(train_data['review'])\n",
    "sentiments = list(train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "031aca75-9218-4204-9773-9c34428b156b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "295dde2a-9bad-460c-b70c-300b781a9b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stuff going moment mj started listening music watching odd documentary watched wiz watched moonwalker maybe want get certain insight guy thought really cool eighties maybe make mind whether guilty innocent moonwalker part biography part feature film remember going see cinema originally released subtle messages mj feeling towards press also obvious message drugs bad kay visually impressive course michael jackson unless remotely like mj anyway going hate find boring may call mj egotist consenting making movie mj fans would say made fans true really nice actual feature film bit finally starts minutes excluding smooth criminal sequence joe pesci convincing psychopathic powerful drug lord wants mj dead bad beyond mj overheard plans nah joe pesci character ranted wanted people know supplying drugs etc dunno maybe hates mj music lots cool things like mj turning car robot whole speed demon sequence also director must patience saint came filming kiddy bad sequence usually directors hate working one kid let alone whole bunch performing complex dance scene bottom line movie people like mj one level another think people stay away try give wholesome message ironically mj bestest buddy movie girl michael jackson truly one talented people ever grace planet guilty well attention gave subject hmmm well know people different behind closed doors know fact either extremely nice stupid guy one sickest liars hope latter\n"
     ]
    }
   ],
   "source": [
    "print(reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9ae94a9-c6ee-45c3-bde2-4554f6a7992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for review in reviews:\n",
    "    sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9cb5128-e864-482b-a9ef-da71171910d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stuff', 'going', 'moment', 'mj', 'started', 'listening', 'music', 'watching', 'odd', 'documentary', 'watched', 'wiz', 'watched', 'moonwalker', 'maybe', 'want', 'get', 'certain', 'insight', 'guy', 'thought', 'really', 'cool', 'eighties', 'maybe', 'make', 'mind', 'whether', 'guilty', 'innocent', 'moonwalker', 'part', 'biography', 'part', 'feature', 'film', 'remember', 'going', 'see', 'cinema', 'originally', 'released', 'subtle', 'messages', 'mj', 'feeling', 'towards', 'press', 'also', 'obvious', 'message', 'drugs', 'bad', 'kay', 'visually', 'impressive', 'course', 'michael', 'jackson', 'unless', 'remotely', 'like', 'mj', 'anyway', 'going', 'hate', 'find', 'boring', 'may', 'call', 'mj', 'egotist', 'consenting', 'making', 'movie', 'mj', 'fans', 'would', 'say', 'made', 'fans', 'true', 'really', 'nice', 'actual', 'feature', 'film', 'bit', 'finally', 'starts', 'minutes', 'excluding', 'smooth', 'criminal', 'sequence', 'joe', 'pesci', 'convincing', 'psychopathic', 'powerful', 'drug', 'lord', 'wants', 'mj', 'dead', 'bad', 'beyond', 'mj', 'overheard', 'plans', 'nah', 'joe', 'pesci', 'character', 'ranted', 'wanted', 'people', 'know', 'supplying', 'drugs', 'etc', 'dunno', 'maybe', 'hates', 'mj', 'music', 'lots', 'cool', 'things', 'like', 'mj', 'turning', 'car', 'robot', 'whole', 'speed', 'demon', 'sequence', 'also', 'director', 'must', 'patience', 'saint', 'came', 'filming', 'kiddy', 'bad', 'sequence', 'usually', 'directors', 'hate', 'working', 'one', 'kid', 'let', 'alone', 'whole', 'bunch', 'performing', 'complex', 'dance', 'scene', 'bottom', 'line', 'movie', 'people', 'like', 'mj', 'one', 'level', 'another', 'think', 'people', 'stay', 'away', 'try', 'give', 'wholesome', 'message', 'ironically', 'mj', 'bestest', 'buddy', 'movie', 'girl', 'michael', 'jackson', 'truly', 'one', 'talented', 'people', 'ever', 'grace', 'planet', 'guilty', 'well', 'attention', 'gave', 'subject', 'hmmm', 'well', 'know', 'people', 'different', 'behind', 'closed', 'doors', 'know', 'fact', 'either', 'extremely', 'nice', 'stupid', 'guy', 'one', 'sickest', 'liars', 'hope', 'latter']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7213085-f3ac-4144-8db6-f819d702ec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300\n",
    "min_word_count = 40\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3a2e8f-80ed-4e75-843a-d7987cf6c5c7",
   "metadata": {},
   "source": [
    "- num_features : 각 단어에 대해 임베딩된 벡터의 차원 지정(feature 수)\n",
    "- min_word_count : 모델에 의미 있는 단어를 가지고 학습하기 위해 적은 빈도 수의 단어들은 학습하지 않기 위해 설정  \n",
    "- num_workers : 모델 학습 시 학습을 위한 쓰레드 수 지정(기본값 3)  \n",
    "- context : word2vec 을 수행하기 위한 컨텍스트 윈도우 사이즈 지정  \n",
    "a. Maximum distance between the current and predicted word within a sentence.  \n",
    "b. 기준 단어의 앞뒤에 존재하는 단어들로 기준 단어를 예측하게 되는데(sg=0, CBOW-Continuous Bag of Words)  \n",
    "c. 이 때 기준 단어에서 앞뒤 얼마나 떨어져 있는 단어까지 고려하는가를 결정\n",
    "- downsampling : word2vec 학습을 수행할 때 빠른 학습을 위해 정답 단어 레이블에 대한 다운샘플링 비율을 지정  \n",
    "a. 보통 0.001이 좋은 성능을 낸다고 알려짐  \n",
    "b. 0.001 값을 threshold 값으로 보고, 이 값보다 빈도수가 높은 단어들은 무작위로(랜덤) 다운샘플링 됨  \n",
    "c. 빈도수가 높은 단어는 다운샘플링하여 가끔 학습(랜덤하게 무시)하고 빈도수가 낮은 단어는 출현 족족 학습하는 효과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39b4cb8d-e87c-4c96-8461-c19d6eae1779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# logging에 레벨이 있음(인포냐, 워닝이냐 ....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "199dd3e2-9362-4535-a30f-665d6c2e145c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-08 14:04:53,736 : INFO : collecting all words and their counts\n",
      "2021-10-08 14:04:53,736 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-10-08 14:04:53,912 : INFO : PROGRESS: at sentence #10000, processed 1205223 words, keeping 51374 word types\n",
      "2021-10-08 14:04:54,093 : INFO : PROGRESS: at sentence #20000, processed 2396605 words, keeping 67660 word types\n",
      "2021-10-08 14:04:54,189 : INFO : collected 74065 word types from a corpus of 2988089 raw words and 25000 sentences\n",
      "2021-10-08 14:04:54,190 : INFO : Loading a fresh vocabulary\n",
      "2021-10-08 14:04:54,229 : INFO : effective_min_count=40 retains 8160 unique words (11% of original 74065, drops 65905)\n",
      "2021-10-08 14:04:54,229 : INFO : effective_min_count=40 leaves 2627273 word corpus (87% of original 2988089, drops 360816)\n",
      "2021-10-08 14:04:54,247 : INFO : deleting the raw counts dictionary of 74065 items\n",
      "2021-10-08 14:04:54,249 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2021-10-08 14:04:54,249 : INFO : downsampling leaves estimated 2494384 word corpus (94.9% of prior 2627273)\n",
      "2021-10-08 14:04:54,269 : INFO : estimated required memory for 8160 words and 300 dimensions: 23664000 bytes\n",
      "2021-10-08 14:04:54,270 : INFO : resetting layer weights\n",
      "2021-10-08 14:04:55,442 : INFO : training model with 4 workers on 8160 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2021-10-08 14:04:56,461 : INFO : EPOCH 1 - PROGRESS: at 44.11% examples, 1092521 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 14:04:57,470 : INFO : EPOCH 1 - PROGRESS: at 96.42% examples, 1190450 words/s, in_qsize 8, out_qsize 0\n",
      "2021-10-08 14:04:57,517 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-08 14:04:57,519 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-08 14:04:57,522 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-08 14:04:57,525 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-08 14:04:57,525 : INFO : EPOCH - 1 : training on 2988089 raw words (2494623 effective words) took 2.1s, 1200038 effective words/s\n",
      "2021-10-08 14:04:58,538 : INFO : EPOCH 2 - PROGRESS: at 52.35% examples, 1301904 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 14:04:59,408 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-08 14:04:59,410 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-08 14:04:59,412 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-08 14:04:59,422 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-08 14:04:59,423 : INFO : EPOCH - 2 : training on 2988089 raw words (2494580 effective words) took 1.9s, 1315767 effective words/s\n",
      "2021-10-08 14:05:00,429 : INFO : EPOCH 3 - PROGRESS: at 49.36% examples, 1238168 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 14:05:01,344 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-08 14:05:01,347 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-08 14:05:01,349 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-08 14:05:01,352 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-08 14:05:01,353 : INFO : EPOCH - 3 : training on 2988089 raw words (2494686 effective words) took 1.9s, 1294573 effective words/s\n",
      "2021-10-08 14:05:02,363 : INFO : EPOCH 4 - PROGRESS: at 52.03% examples, 1298586 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 14:05:03,280 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-08 14:05:03,284 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-08 14:05:03,286 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-08 14:05:03,296 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-08 14:05:03,297 : INFO : EPOCH - 4 : training on 2988089 raw words (2494286 effective words) took 1.9s, 1284727 effective words/s\n",
      "2021-10-08 14:05:04,310 : INFO : EPOCH 5 - PROGRESS: at 52.93% examples, 1318882 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 14:05:05,172 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-08 14:05:05,178 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-08 14:05:05,182 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-08 14:05:05,185 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-08 14:05:05,186 : INFO : EPOCH - 5 : training on 2988089 raw words (2494352 effective words) took 1.9s, 1322590 effective words/s\n",
      "2021-10-08 14:05:05,187 : INFO : training on a 14940445 raw words (12472527 effective words) took 9.7s, 1280065 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, size=num_features, min_count=min_word_count, window=context, sample=downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "876d9fa1-c00a-4aaf-9b2b-022056724db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-08 14:05:05,194 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2021-10-08 14:05:05,195 : INFO : not storing attribute vectors_norm\n",
      "2021-10-08 14:05:05,196 : INFO : not storing attribute cum_table\n",
      "2021-10-08 14:05:05,313 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4734da1-369a-431e-9420-d4534768f85f",
   "metadata": {},
   "source": [
    "- 위에서 만든 word2vec 모델을 활용하여 선형 회귀 모델을 학습시켜 봄\n",
    "- 각 리뷰를 같은 형태의 입력값으로 만들어야 함\n",
    "- 리뷰마다 단어의 수가 모두 다르므로 입력값을 하나의 형태로 만듦\n",
    "- 가장 단순한 방법으로, 문장에 있는 모든 단어의 벡터값에 대해 평균을 내서 리뷰 하나당 하나의 벡터로 만드는 방법을 사용하겠음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc6443d2-51c6-4ec5-8ab1-79f33575db11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(words, model, num_features):\n",
    "    feature_vector = np.zeros((num_features), dtype=np.float32)\n",
    "    \n",
    "    num_words = 0\n",
    "    index2word_set = set(model.wv.index2word) # 단어사전\n",
    "    \n",
    "    for w in words:\n",
    "        if w in index2word_set: #단어사전에 존재하는지 확인\n",
    "            num_words += 1   # 카운트 하나 늘려서\n",
    "            feature_vector = np.add(feature_vector, model[w])  #해당단어의 벡터값을 꺼내서 더해줌\n",
    "            \n",
    "    feature_vector = np.divide(feature_vector, num_words)   # 카운트한 단어의 값으로 나눠줘서 평균으로 만들어줌\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6d2654-c1d2-4731-a6fb-6927f3d899bd",
   "metadata": {},
   "source": [
    "- words : 단어의 모음인 하나의 리뷰가 들어감\n",
    "- model : word2vec 모델\n",
    "- num_features : word2vec 으로 임베딩할 때 정했던 벡터의 차원 수  \n",
    "- 결국 하나의 문장에 등장하는 사전에 등록된 단어들의 벡터값의 평균을 구함  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1898a212-a894-424b-bda4-f37240d5a440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(reviews, model, num_features):\n",
    "    dataset = list()\n",
    "    \n",
    "    for s in reviews:   # 전체 리뷰를 돌려서\n",
    "        dataset.append(get_feature(s, model, num_features))\n",
    "        \n",
    "    reviewFeatureVecs = np.stack(dataset)  # stack: dataset을 low로 쌓으면서 넘파이 배열을 만듬\n",
    "    \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdb7ab2-3a09-45a3-8193-dbf289d933da",
   "metadata": {},
   "source": [
    "- review : 전체 리뷰 데이터  \n",
    "- model : 학습시킨 모델  \n",
    "- num_features : word2vec 임베딩 시 정했던 벡터의 차원 수  \n",
    "- np.stack(dataset, axis=0) 은 row 로 데이터를 쌓으면서 numpy 배열을 만든다는 의미  \n",
    "- 이렇게 하여 row가 전체 샘플 수 만큼, column 은 feature의 차원 수가 됨  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e590045-68a8-4d0b-babb-8c792157ce7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\relea\\AppData\\Local\\Temp/ipykernel_5156/2429604096.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  feature_vector = np.add(feature_vector, model[w])  #해당단어의 벡터값을 꺼내서 더해줌\n"
     ]
    }
   ],
   "source": [
    "train_data_vecs = get_dataset(sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a93b8e0-d8c0-4ac5-b15d-12adfa5d030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train_data_vecs\n",
    "y = np.array(sentiments)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "TEST_SPLIT = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RANDOM_SEED) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "806bb90b-4b47-4d35-9076-07d6bebd9748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\relea\\.conda\\envs\\python-env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lgs = LogisticRegression(class_weight='balanced')\n",
    "lgs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b615aed9-d5b3-4407-9fa2-d03d17a9fc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.862800\n"
     ]
    }
   ],
   "source": [
    "predicted = lgs.predict(X_test)\n",
    "print('Accuracy: %f' % lgs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dab48eab-caae-4474-876c-e6726e3c9d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CLEAN_DATA = 'test_clean.csv'\n",
    "\n",
    "test_data = pd.read_csv(DATA_IN_PATH + TEST_CLEAN_DATA)\n",
    "test_review = list(test_data['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "faa3ab90-7de1-4099-83ea-5f93e9e2f3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = list()\n",
    "for review in test_review:\n",
    "    test_sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82eea238-e526-46d9-bdaa-83ed90330d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\relea\\AppData\\Local\\Temp/ipykernel_5156/2429604096.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  feature_vector = np.add(feature_vector, model[w])  #해당단어의 벡터값을 꺼내서 더해줌\n"
     ]
    }
   ],
   "source": [
    "test_data_vecs = get_dataset(test_sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e4353e0-2899-4c13-9814-7b83d582720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted = lgs.predict(test_data_vecs)\n",
    "\n",
    "ids = list(test_data['id'])\n",
    "answer_dataset = pd.DataFrame({'id': ids, 'sentiment':test_predicted})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be0c1826-fc7f-4c61-b94e-e553fe4c7411",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "    \n",
    "answer_dataset.to_csv(DATA_OUT_PATH + 'lgs_w2v_answer.csv', index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18996246-c37e-45fa-94d5-80591855ae2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
